{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f23348d4",
   "metadata": {},
   "source": [
    "# Cleaning code for Building 59 dataset\n",
    "\n",
    "The .csv files from the dataset are located on the path declared right below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a650843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from fancyimpute import KNN, MatrixFactorization\n",
    "import math\n",
    "\n",
    "path = \"../data\" #Path with raw csv files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca3c33",
   "metadata": {},
   "source": [
    "This is the code presented on the paper, we are not able to execute it due to RAM problems, so we will try to transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b724d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_data_from_path(path):\n",
    "    files = os.listdir(path)\n",
    "    path_postprocess = path + \"_postprocess\"\n",
    "\n",
    "    #read data files and adjust time format\n",
    "    for filename in files:\n",
    "        print(path+'/'+filename)\n",
    "        df = pd.read_csv(path+'/'+filename)\n",
    "        df['date'] = pd.to_datetime(df['date']) \n",
    "        helper=pd.DataFrame({'date': pd.date_range(df['date'].min(), df['date'].max(), freq='15min')})\n",
    "        df = pd.merge(df, helper, on='date', how='outer').sort_values('date')\n",
    "        count_out = Series([0],index=['date']) #count of outlier values\n",
    "        count_gap = Series([0],index=['date']) #count of gap\n",
    "        count_outgap = Series([0],index=['date']) #count of large gap (e.g., one day)\n",
    "        gap_max=Series([0],index=['date']) #maximum gap\n",
    "        #calculate the count of gap and do the interpolation based on the gap size \n",
    "        for i in range(1, len(df.columns)):\n",
    "            k = 0\n",
    "            out_gapcount=0\n",
    "            start_index = {}\n",
    "            starttime = {}\n",
    "            end_index = {}\n",
    "            endtime = {}\n",
    "            gap = {}\n",
    "            \n",
    "    \n",
    "            if pd.isnull(df.iloc[len(df.index)-1,i]) == True or math.isnan(df.iloc[len(df.index)-1,i])==True:\n",
    "                df.iloc[len(df.index)-1,i]=0\n",
    "            for j in range(0, len(df.index)):\n",
    "                if (pd.isnull(df.iloc[j,i]) or math.isnan(df.iloc[j,i]))and pd.isnull(df.iloc[j-1,i]) == False:\n",
    "                    starttime[k]=df.iloc[j-1,0] #start time of the gap\n",
    "                    start_index[k]=j-1\n",
    "                elif (pd.isnull(df.iloc[j-1,i]) or math.isnan(df.iloc[j-1,i])) and pd.isnull(df.iloc[j,i]) == False:\n",
    "                    endtime[k]=df.iloc[j,0] #end time of the gap\n",
    "                    end_index[k]=j\n",
    "                    k=k+1\n",
    "            if k != 0:\n",
    "                for m in range(k):\n",
    "                    starttime_struct=datetime.datetime.strptime(str(starttime[m]), '%Y-%m-%d %H:%M:%S')\n",
    "                    endtime_struct = datetime.datetime.strptime(str(endtime[m]), '%Y-%m-%d %H:%M:%S')\n",
    "                    gap[m]=(endtime_struct-starttime_struct).total_seconds()\n",
    "                    if  gap[m]<= 3600: #linear interpolation if the gap is less than one hour\n",
    "                        df.iloc[start_index[m]:end_index[m]+1,i]=df.iloc[start_index[m]:end_index[m]+1,i].interpolate(method='linear')\n",
    "                    elif gap[m] >3600*24:\n",
    "                        out_gapcount=out_gapcount+1\n",
    "                maxgap = max(gap.values())/60\n",
    "                gap_max=gap_max.append(Series(maxgap,index=[df.columns[i]]))\n",
    "            outcount=np.sum(df.iloc[:, i]<0)/len(df)\n",
    "            count_out=count_out.append(Series(outcount, index=[df.columns[i]]))\n",
    "            count_gap= count_gap.append(Series(k, index=[df.columns[i]]))\n",
    "            count_outgap = count_outgap.append(Series(out_gapcount,index=[df.columns[i]]))\n",
    "            df_interpolation=np.array(df.iloc[:,1:])\n",
    "        df_interpolation= KNN(k=3).fit_transform(df_interpolation) #Apply knn algorithm if the gap is larger than one hour\n",
    "        unfill_large_gaps(df_filled, df)\n",
    "        if out_gapcount !=0:\n",
    "            df_interpolation= MatrixFactorization().fit_transform(df_interpolation) #Apply MF algorithm if the gap is larger than one day         \n",
    "        df.iloc[:,1:]=df_interpolation\n",
    "        cols_not_null = (len(df)-df.count(axis=0))/len(df)\n",
    "        data=pd.DataFrame({'missingrate':cols_not_null,'outrate':count_out,'count_outgap':count_outgap,'count_gap':count_gap,'maxgap':gap_max})\n",
    "        data.to_csv(path_postprocess+'\\\\'+'parameter_'+filename, sep=',', header=True, index=True)\n",
    "        df.to_csv(path_postprocess+'\\\\'+'data_'+filename, sep=',', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb23f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5b6cbd0",
   "metadata": {},
   "source": [
    "We have a problem with ele.csv (energy use), because it doesn't follow the same csv format as the other files: it includes an unnamed column without data. We solve this problem with the following code (run only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c038c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datos = pd.read_csv(path+ '/ele.csv')\n",
    "#datos.drop('Unnamed: 6', axis=1, inplace=True)\n",
    "#datos = datos.set_index('date')\n",
    "#datos.to_csv(path+ '/ele.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a079a699",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/zone_temp_sp_c.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bj/wg2_mprn1fl8bf0h7tq_sh2c0000gn/T/ipykernel_29543/3073045708.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The same problem appears in zone_temp_sp_h and zone_temp_sp_c (unnamed features, all of which have more than 50% missing data), we will just drop then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# We also drop the column zone_070_cooling_sp because it has over 97% of missing values in the interval we will consider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdatos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'/zone_temp_sp_c.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdatos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdatos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/zone_temp_sp_c.csv'"
     ]
    }
   ],
   "source": [
    "# The same problem appears in zone_temp_sp_h and zone_temp_sp_c (unnamed features, all of which have more than 50% missing data), we will just drop then\n",
    "# We also drop the column zone_070_cooling_sp because it has over 97% of missing values in the interval we will consider\n",
    "datos = pd.read_csv(path+ '/zone_temp_sp_c.csv')\n",
    "datos = datos.iloc[:,:40]\n",
    "datos = datos.set_index('date')\n",
    "#datos.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "datos.to_csv(path+ '/zone_temp_sp_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7823cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv(path+ '/zone_temp_sp_h.csv')\n",
    "datos = datos.iloc[:,:40]\n",
    "datos = datos.set_index('date')\n",
    "#datos.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "datos.to_csv(path+ '/zone_temp_sp_h.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed67d0",
   "metadata": {},
   "source": [
    "### Study of null values by column\n",
    "For a file, we will study the percentage of missing values it includes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fbc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(path, filename):\n",
    "    print(\"SUMMARY OF \" + filename)\n",
    "    dataframe = pd.read_csv(path+'/'+filename)\n",
    "    dataframe=dataframe.set_index('date')\n",
    "    for i in range(dataframe.shape[1]):\n",
    "        print(dataframe.columns[i])\n",
    "        n_miss = dataframe.iloc[:,i].isnull().sum()\n",
    "        perc = n_miss / dataframe.shape[0] * 100\n",
    "        print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))\n",
    "\n",
    "def summary2(dataframe):\n",
    "    for i in range(dataframe.shape[1]):\n",
    "        print(dataframe.columns[i])\n",
    "        n_miss = dataframe.iloc[:,i].isnull().sum()\n",
    "        perc = n_miss / dataframe.shape[0] * 100\n",
    "        print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd0e250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY OF wifi.csv\n",
      "wifi_first_south\n",
      "> 0, Missing: 7344 (7.3%)\n",
      "wifi_second_south\n",
      "> 1, Missing: 7344 (7.3%)\n",
      "wifi_third_south\n",
      "> 2, Missing: 0 (0.0%)\n",
      "wifi_fourth_south\n",
      "> 3, Missing: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "summary(path, 'wifi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac6d86",
   "metadata": {},
   "source": [
    "## Interpolation depending on the size of the gap:\n",
    "\n",
    "- If it's smaller than 1h, we use linear interpolation\n",
    "- If it's bigger than 1 day, we use KNN with n=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6c7ce",
   "metadata": {},
   "source": [
    "Interpolation with KNN: \n",
    "\n",
    "https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa9229fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "files = os.listdir(path)\n",
    "path_postprocess = path + \"_postprocess\"\n",
    "#in the cleaning code we insert the rows that are apparently missing\n",
    "\n",
    "freqs = {'zone_co2.csv':'1min', 'ele.csv': '15min', 'zone_temp_sp_c.csv':'5min', 'occ.csv':'1min', 'zone_temp_exterior.csv':'1min', 'zone_temp_sp_h.csv':'5min', 'site_weather.csv':'15min', 'wifi.csv': '10min', 'zone_temp_interior.csv':'10min'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37749eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfill_large_gaps(df_filled, df):\n",
    "    \n",
    "    for i in range(1, len(df.columns)):\n",
    "        k=0\n",
    "        start_index = {}\n",
    "        starttime = {}\n",
    "        end_index = {}\n",
    "        endtime = {}\n",
    "        gap={}\n",
    "        for j in range(0, len(df.index)):\n",
    "            if pd.isnull(df.iloc[j,i]) and pd.isnull(df.iloc[j-1,i]) == False:\n",
    "                starttime[k]=df.iloc[j-1,0]\n",
    "                start_index[k]=j-1\n",
    "            elif pd.isnull(df.iloc[j-1,i]) and pd.isnull(df.iloc[j,i]) == False:\n",
    "                endtime[k]=df.iloc[j,0]\n",
    "                end_index[k]=j\n",
    "                k=k+1\n",
    "        for m in range(k):\n",
    "            starttime_struct=datetime.datetime.strptime(str(starttime[m]), '%Y-%m-%d %H:%M:%S')\n",
    "            endtime_struct = datetime.datetime.strptime(str(endtime[m]), '%Y-%m-%d %H:%M:%S')\n",
    "            gap[m]=(endtime_struct-starttime_struct).total_seconds()\n",
    "            if  gap[m]>= 3600*24:\n",
    "                df_filled.iloc[start_index[m]:end_index[m]+1,i-1]= None\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc6da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_csv(path, filename, freq):\n",
    "    print(path+'/'+filename)\n",
    "    path_postprocess = path+\"_postprocess/data_definitivo\"+'/'+filename[:-4]+ \"_postprocess.csv\"\n",
    "    df = pd.read_csv(path+'/'+filename)\n",
    "    df['date'] = pd.to_datetime(df['date']) \n",
    "    helper=pd.DataFrame({'date': pd.date_range(df['date'].min(), df['date'].max(), freq=freq)})\n",
    "    df = pd.merge(df, helper, on='date', how='outer').sort_values('date')\n",
    "    count_out = Series([0],index=['date']) #count of outlier values\n",
    "    count_gap = Series([0],index=['date']) #count of gap\n",
    "    count_outgap = Series([0],index=['date']) #count of large gap (e.g., one day)\n",
    "    gap_max=Series([0],index=['date']) #maximum gap\n",
    "    out_gapcount=0\n",
    "    summary2(df)\n",
    "    #calculate the count of gap and do the interpolation based on the gap size \n",
    "    for i in range(1, len(df.columns)):\n",
    "        print(\"Estamos en: \", i)\n",
    "        k = 0\n",
    "        \n",
    "        start_index = {}\n",
    "        starttime = {}\n",
    "        end_index = {}\n",
    "        endtime = {}\n",
    "        gap = {}\n",
    "        if pd.isnull(df.iloc[len(df.index)-1,i]) == True or math.isnan(df.iloc[len(df.index)-1,i])==True:\n",
    "            df.iloc[len(df.index)-1,i]=0\n",
    "        for j in range(0, len(df.index)):\n",
    "            if (pd.isnull(df.iloc[j,i]) or math.isnan(df.iloc[j,i]))and pd.isnull(df.iloc[j-1,i]) == False:\n",
    "                starttime[k]=df.iloc[j-1,0] #start time of the gap\n",
    "                start_index[k]=j-1\n",
    "            elif (pd.isnull(df.iloc[j-1,i]) or math.isnan(df.iloc[j-1,i])) and pd.isnull(df.iloc[j,i]) == False:\n",
    "                endtime[k]=df.iloc[j,0] #end time of the gap\n",
    "                end_index[k]=j\n",
    "                k=k+1\n",
    "        if k != 0:\n",
    "            for m in range(k):\n",
    "                starttime_struct=datetime.datetime.strptime(str(starttime[m]), '%Y-%m-%d %H:%M:%S')\n",
    "                endtime_struct = datetime.datetime.strptime(str(endtime[m]), '%Y-%m-%d %H:%M:%S')\n",
    "                gap[m]=(endtime_struct-starttime_struct).total_seconds()\n",
    "                if  gap[m]<= 3600: #linear interpolation if the gap is less than one hour\n",
    "                    print(\"Interpolation linear\")\n",
    "                    df.iloc[start_index[m]:end_index[m]+1,i]=df.iloc[start_index[m]:end_index[m]+1,i].interpolate(method='linear')\n",
    "                elif gap[m] >3600*24:\n",
    "                    out_gapcount=out_gapcount+1\n",
    "            maxgap = max(gap.values())/60\n",
    "            gap_max=gap_max.append(Series(maxgap,index=[df.columns[i]]))\n",
    "        outcount=np.sum(df.iloc[:, i]<0)/len(df)\n",
    "        count_out=count_out.append(Series(outcount, index=[df.columns[i]]))\n",
    "        count_gap= count_gap.append(Series(k, index=[df.columns[i]]))\n",
    "        count_outgap = count_outgap.append(Series(out_gapcount,index=[df.columns[i]]))\n",
    "    #Interpolate whole dataframe with KNN\n",
    "    df_interpolated = df.iloc[:,1:]\n",
    "    imputer = KNNImputer(n_neighbors=3, weights='distance', metric='nan_euclidean')\n",
    "    imputer.fit(df_interpolated)\n",
    "    df_interpolated = pd.DataFrame(imputer.transform(df_interpolated), columns=df_interpolated.columns)\n",
    "\n",
    "    \n",
    "    #Export into csv\n",
    "    print(\"New summary(final): \")\n",
    "    summary2(df_interpolated)\n",
    "    df.iloc[:,1:] = df_interpolated\n",
    "    df.to_csv(path_postprocess, sep=',', header=True, index=False)\n",
    "\n",
    "    #Final set of information\n",
    "    cols_not_null = (len(df)-df.count(axis=0))/len(df)\n",
    "    data=pd.DataFrame({'missingrate':cols_not_null,'outrate':count_out,'count_outgap':count_outgap,'count_gap':count_gap,'maxgap':gap_max})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29135a0",
   "metadata": {},
   "source": [
    "However, we have a problem with available data. To solve this problem, we will only keep values between '2018-05-22 07:00:00'\n",
    "and '2019-02-21 10:11:00' (maximum interval where all the variables are available), in order to do so, we will \"crop\" the data before interpolating it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b64845cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>zone_016_cooling_sp</th>\n",
       "      <th>zone_017_cooling_sp</th>\n",
       "      <th>zone_018_cooling_sp</th>\n",
       "      <th>zone_019_cooling_sp</th>\n",
       "      <th>zone_021_cooling_sp</th>\n",
       "      <th>zone_023_cooling_sp</th>\n",
       "      <th>zone_024_cooling_sp</th>\n",
       "      <th>zone_025_cooling_sp</th>\n",
       "      <th>...</th>\n",
       "      <th>zone_059_cooling_sp</th>\n",
       "      <th>zone_061_cooling_sp</th>\n",
       "      <th>zone_062_cooling_sp</th>\n",
       "      <th>zone_063_cooling_sp</th>\n",
       "      <th>zone_064_cooling_sp</th>\n",
       "      <th>zone_065_cooling_sp</th>\n",
       "      <th>zone_066_cooling_sp</th>\n",
       "      <th>zone_067_cooling_sp</th>\n",
       "      <th>zone_069_cooling_sp</th>\n",
       "      <th>zone_070_cooling_sp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018/9/15 10:00</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018/9/15 10:05</td>\n",
       "      <td>1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018/9/15 10:10</td>\n",
       "      <td>2</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  Unnamed: 0  zone_016_cooling_sp  zone_017_cooling_sp  \\\n",
       "0  2018/9/15 10:00           0                 73.0                 73.0   \n",
       "1  2018/9/15 10:05           1                 73.0                 73.0   \n",
       "2  2018/9/15 10:10           2                 73.0                 73.0   \n",
       "\n",
       "   zone_018_cooling_sp  zone_019_cooling_sp  zone_021_cooling_sp  \\\n",
       "0                 75.0                 78.0                 75.0   \n",
       "1                 75.0                 78.0                 75.0   \n",
       "2                 75.0                 78.0                 75.0   \n",
       "\n",
       "   zone_023_cooling_sp  zone_024_cooling_sp  zone_025_cooling_sp  ...  \\\n",
       "0                 73.0                 73.0                 73.0  ...   \n",
       "1                 73.0                 73.0                 73.0  ...   \n",
       "2                 73.0                 73.0                 73.0  ...   \n",
       "\n",
       "   zone_059_cooling_sp  zone_061_cooling_sp  zone_062_cooling_sp  \\\n",
       "0                 73.0                 69.0                 69.0   \n",
       "1                 73.0                 69.0                 69.0   \n",
       "2                 73.0                 69.0                 69.0   \n",
       "\n",
       "   zone_063_cooling_sp  zone_064_cooling_sp  zone_065_cooling_sp  \\\n",
       "0                 75.0                 73.0                 73.0   \n",
       "1                 75.0                 73.0                 73.0   \n",
       "2                 75.0                 73.0                 73.0   \n",
       "\n",
       "   zone_066_cooling_sp  zone_067_cooling_sp  zone_069_cooling_sp  \\\n",
       "0                 73.0                 73.0                 75.0   \n",
       "1                 73.0                 73.0                 75.0   \n",
       "2                 73.0                 73.0                 75.0   \n",
       "\n",
       "   zone_070_cooling_sp  \n",
       "0                 75.0  \n",
       "1                 75.0  \n",
       "2                 75.0  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datos = pd.read_csv(path+ '/zone_temp_sp_c.csv')\n",
    "datos.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a2f1c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>occ_third_south</th>\n",
       "      <th>occ_fourth_south</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-22 07:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-22 07:01:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-22 07:02:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-22 07:03:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-22 07:04:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396187</th>\n",
       "      <td>2019-02-21 10:07:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396188</th>\n",
       "      <td>2019-02-21 10:08:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396189</th>\n",
       "      <td>2019-02-21 10:09:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396190</th>\n",
       "      <td>2019-02-21 10:10:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396191</th>\n",
       "      <td>2019-02-21 10:11:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396192 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  occ_third_south  occ_fourth_south\n",
       "0       2018-05-22 07:00:00              0.0               0.0\n",
       "1       2018-05-22 07:01:00              0.0               1.0\n",
       "2       2018-05-22 07:02:00              0.0               0.0\n",
       "3       2018-05-22 07:03:00              0.0               1.0\n",
       "4       2018-05-22 07:04:00              0.0               0.0\n",
       "...                     ...              ...               ...\n",
       "396187  2019-02-21 10:07:00              0.0               0.0\n",
       "396188  2019-02-21 10:08:00              0.0               0.0\n",
       "396189  2019-02-21 10:09:00              0.0               0.0\n",
       "396190  2019-02-21 10:10:00              0.0               0.0\n",
       "396191  2019-02-21 10:11:00              0.0               0.0\n",
       "\n",
       "[396192 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos = pd.read_csv(path+ '/occ.csv')\n",
    "datos.head(-1) #Decidir si nos quedamos con este límite o pasamos de los datos de ocupación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a79e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_csv_cortado(path, filename, freq, start, end):\n",
    "    print(path+'/'+filename)\n",
    "    path_postprocess = path+\"_postprocess/data_definitivo\"+'/'+filename[:-4]+ \"_postprocess.csv\"\n",
    "    df = pd.read_csv(path+'/'+filename)\n",
    "    df['date'] = pd.to_datetime(df['date']) \n",
    "    df =df[(df.date>=start)&(df.date<=end)]\n",
    "    helper=pd.DataFrame({'date': pd.date_range(start, end, freq=freq)})\n",
    "    df = pd.merge(df, helper, on='date', how='outer').sort_values('date')\n",
    "    count_out = Series([0],index=['date']) #count of outlier values\n",
    "    count_gap = Series([0],index=['date']) #count of gap\n",
    "    count_outgap = Series([0],index=['date']) #count of large gap (e.g., one day)\n",
    "    gap_max=Series([0],index=['date']) #maximum gap\n",
    "    out_gapcount=0\n",
    "    summary2(df)\n",
    "    #calculate the count of gap and do the interpolation based on the gap size \n",
    "    for i in range(1, len(df.columns)):\n",
    "        print(\"Estamos en: \", i)\n",
    "        k = 0\n",
    "        \n",
    "        start_index = {}\n",
    "        starttime = {}\n",
    "        end_index = {}\n",
    "        endtime = {}\n",
    "        gap = {}\n",
    "        if pd.isnull(df.iloc[len(df.index)-1,i]) == True or math.isnan(df.iloc[len(df.index)-1,i])==True:\n",
    "            df.iloc[len(df.index)-1,i]=0\n",
    "        for j in range(0, len(df.index)):\n",
    "            if (pd.isnull(df.iloc[j,i]) or math.isnan(df.iloc[j,i]))and pd.isnull(df.iloc[j-1,i]) == False:\n",
    "                starttime[k]=df.iloc[j-1,0] #start time of the gap\n",
    "                start_index[k]=j-1\n",
    "            elif (pd.isnull(df.iloc[j-1,i]) or math.isnan(df.iloc[j-1,i])) and pd.isnull(df.iloc[j,i]) == False:\n",
    "                endtime[k]=df.iloc[j,0] #end time of the gap\n",
    "                end_index[k]=j\n",
    "                k=k+1\n",
    "        if k != 0:\n",
    "            for m in range(k):\n",
    "                starttime_struct=datetime.datetime.strptime(str(starttime[m]), '%Y-%m-%d %H:%M:%S')\n",
    "                endtime_struct = datetime.datetime.strptime(str(endtime[m]), '%Y-%m-%d %H:%M:%S')\n",
    "                gap[m]=(endtime_struct-starttime_struct).total_seconds()\n",
    "                if  gap[m]<= 3600: #linear interpolation if the gap is less than one hour\n",
    "                    print(\"Interpolation linear\")\n",
    "                    df.iloc[start_index[m]:end_index[m]+1,i]=df.iloc[start_index[m]:end_index[m]+1,i].interpolate(method='linear')\n",
    "                elif gap[m] >3600*24:\n",
    "                    out_gapcount=out_gapcount+1\n",
    "            maxgap = max(gap.values())/60\n",
    "            gap_max=gap_max.append(Series(maxgap,index=[df.columns[i]]))\n",
    "        outcount=np.sum(df.iloc[:, i]<0)/len(df)\n",
    "        count_out=count_out.append(Series(outcount, index=[df.columns[i]]))\n",
    "        count_gap= count_gap.append(Series(k, index=[df.columns[i]]))\n",
    "        count_outgap = count_outgap.append(Series(out_gapcount,index=[df.columns[i]]))\n",
    "    #Interpolate whole dataframe with KNN\n",
    "    df_interpolated = df.iloc[:,1:]\n",
    "    imputer = KNNImputer(n_neighbors=3, weights='distance', metric='nan_euclidean')\n",
    "    imputer.fit(df_interpolated)\n",
    "    df_interpolated = pd.DataFrame(imputer.transform(df_interpolated), columns=df_interpolated.columns)\n",
    "\n",
    "    \n",
    "    #Export into csv\n",
    "    print(\"New summary(final): \")\n",
    "    summary2(df_interpolated)\n",
    "    df.iloc[:,1:] = df_interpolated\n",
    "    df.to_csv(path_postprocess, sep=',', header=True, index=False)\n",
    "\n",
    "    #Final set of information\n",
    "    cols_not_null = (len(df)-df.count(axis=0))/len(df)\n",
    "    data=pd.DataFrame({'missingrate':cols_not_null,'outrate':count_out,'count_outgap':count_outgap,'count_gap':count_gap,'maxgap':gap_max})\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09bb6629",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '2018-09-15 10:00:00'\n",
    "end = '2019-02-21 10:11:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f45c59b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_temp_exterior.csv 1min\n",
      "../data/zone_temp_exterior.csv\n",
      "date\n",
      "> 0, Missing: 0 (0.0%)\n",
      "zone_016_temp\n",
      "> 1, Missing: 43431 (19.0%)\n",
      "zone_017_temp\n",
      "> 2, Missing: 43425 (19.0%)\n",
      "zone_018_temp\n",
      "> 3, Missing: 43427 (19.0%)\n",
      "zone_019_temp\n",
      "> 4, Missing: 45933 (20.1%)\n",
      "zone_021_temp\n",
      "> 5, Missing: 43044 (18.8%)\n",
      "zone_022_temp\n",
      "> 6, Missing: 44384 (19.4%)\n",
      "zone_023_temp\n",
      "> 7, Missing: 44464 (19.4%)\n",
      "zone_024_temp\n",
      "> 8, Missing: 44514 (19.4%)\n",
      "zone_025_temp\n",
      "> 9, Missing: 45967 (20.1%)\n",
      "zone_026_temp\n",
      "> 10, Missing: 43138 (18.8%)\n",
      "zone_027_temp\n",
      "> 11, Missing: 46276 (20.2%)\n",
      "zone_028_temp\n",
      "> 12, Missing: 45861 (20.0%)\n",
      "zone_030_temp\n",
      "> 13, Missing: 43475 (19.0%)\n",
      "zone_032_temp\n",
      "> 14, Missing: 46392 (20.3%)\n",
      "zone_033_temp\n",
      "> 15, Missing: 45929 (20.1%)\n",
      "zone_035_temp\n",
      "> 16, Missing: 43068 (18.8%)\n",
      "zone_036_temp\n",
      "> 17, Missing: 43469 (19.0%)\n",
      "zone_037_temp\n",
      "> 18, Missing: 43437 (19.0%)\n",
      "zone_038_temp\n",
      "> 19, Missing: 43475 (19.0%)\n",
      "zone_039_temp\n",
      "> 20, Missing: 43453 (19.0%)\n",
      "zone_040_temp\n",
      "> 21, Missing: 45897 (20.0%)\n",
      "zone_041_temp\n",
      "> 22, Missing: 43185 (18.9%)\n",
      "zone_042_temp\n",
      "> 23, Missing: 43468 (19.0%)\n",
      "zone_043_temp\n",
      "> 24, Missing: 43480 (19.0%)\n",
      "zone_044_temp\n",
      "> 25, Missing: 43405 (19.0%)\n",
      "zone_045_temp\n",
      "> 26, Missing: 43031 (18.8%)\n",
      "zone_046_temp\n",
      "> 27, Missing: 43449 (19.0%)\n",
      "zone_047_temp\n",
      "> 28, Missing: 46274 (20.2%)\n",
      "zone_048_temp\n",
      "> 29, Missing: 32433 (14.2%)\n",
      "zone_049_temp\n",
      "> 30, Missing: 46286 (20.2%)\n",
      "zone_051_temp\n",
      "> 31, Missing: 45584 (19.9%)\n",
      "zone_052_temp\n",
      "> 32, Missing: 45893 (20.0%)\n",
      "zone_053_temp\n",
      "> 33, Missing: 46950 (20.5%)\n",
      "zone_054_temp\n",
      "> 34, Missing: 46947 (20.5%)\n",
      "zone_055_temp\n",
      "> 35, Missing: 46935 (20.5%)\n",
      "zone_056_temp\n",
      "> 36, Missing: 46949 (20.5%)\n",
      "zone_057_temp\n",
      "> 37, Missing: 46927 (20.5%)\n",
      "zone_058_temp\n",
      "> 38, Missing: 43817 (19.1%)\n",
      "zone_059_temp\n",
      "> 39, Missing: 46282 (20.2%)\n",
      "zone_061_temp\n",
      "> 40, Missing: 46241 (20.2%)\n",
      "zone_062_temp\n",
      "> 41, Missing: 45956 (20.1%)\n",
      "zone_063_temp\n",
      "> 42, Missing: 45889 (20.0%)\n",
      "zone_064_temp\n",
      "> 43, Missing: 47650 (20.8%)\n",
      "zone_065_temp\n",
      "> 44, Missing: 43767 (19.1%)\n",
      "zone_066_temp\n",
      "> 45, Missing: 46571 (20.3%)\n",
      "zone_067_temp\n",
      "> 46, Missing: 46574 (20.3%)\n",
      "zone_068_temp\n",
      "> 47, Missing: 46957 (20.5%)\n",
      "zone_069_temp\n",
      "> 48, Missing: 45902 (20.0%)\n",
      "zone_070_temp\n",
      "> 49, Missing: 223213 (97.5%)\n",
      "zone_071_temp\n",
      "> 50, Missing: 46566 (20.3%)\n",
      "zone_072_temp\n",
      "> 51, Missing: 46610 (20.4%)\n",
      "Estamos en:  1\n",
      "Estamos en:  2\n",
      "Estamos en:  3\n",
      "Estamos en:  4\n",
      "Estamos en:  5\n",
      "Estamos en:  6\n",
      "Estamos en:  7\n",
      "Estamos en:  8\n",
      "Estamos en:  9\n",
      "Estamos en:  10\n",
      "Estamos en:  11\n",
      "Estamos en:  12\n",
      "Estamos en:  13\n",
      "Estamos en:  14\n",
      "Estamos en:  15\n",
      "Estamos en:  16\n",
      "Estamos en:  17\n",
      "Estamos en:  18\n",
      "Estamos en:  19\n",
      "Estamos en:  20\n",
      "Estamos en:  21\n",
      "Estamos en:  22\n",
      "Estamos en:  23\n",
      "Estamos en:  24\n",
      "Estamos en:  25\n",
      "Estamos en:  26\n",
      "Estamos en:  27\n",
      "Estamos en:  28\n",
      "Estamos en:  29\n",
      "Estamos en:  30\n",
      "Estamos en:  31\n",
      "Estamos en:  32\n",
      "Estamos en:  33\n",
      "Estamos en:  34\n",
      "Estamos en:  35\n",
      "Estamos en:  36\n",
      "Estamos en:  37\n",
      "Estamos en:  38\n",
      "Estamos en:  39\n",
      "Estamos en:  40\n",
      "Estamos en:  41\n",
      "Estamos en:  42\n",
      "Estamos en:  43\n",
      "Estamos en:  44\n",
      "Estamos en:  45\n",
      "Estamos en:  46\n",
      "Estamos en:  47\n",
      "Estamos en:  48\n",
      "Estamos en:  49\n",
      "Estamos en:  50\n",
      "Estamos en:  51\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(path)\n",
    "path_postprocess = path + \"_postprocess\"\n",
    "\n",
    "for file in files:\n",
    "    if file != '.DS_Store':\n",
    "        print(file, freqs[file])\n",
    "        get_csv_cortado(path, file, freqs[file], start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9c3fb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237126, 39)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooling_a = pd.read_csv(path+'/zone_temp_sp_h.csv')\n",
    "cooling_a.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6573c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=cooling_a\n",
    "df.date=pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3884592",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df[(df.date>=start)&(df.date<=end)]\n",
    "helper=pd.DataFrame({'date': pd.date_range(start, end, freq='5min')})\n",
    "df2 = pd.merge(df, helper, on='date', how='outer').sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f77d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "877091ae0c5489928d44a352f429e2e80293f5d77943e3e0fbb4e56d198d5afd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
